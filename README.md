WSG for llama.cpp Server to switch in between two models for Chat and Code completion using Continue extention in VSCode
